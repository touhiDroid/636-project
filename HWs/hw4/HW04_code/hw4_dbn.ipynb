{
 "nbformat": 4,
 "nbformat_minor": 1,
 "cells": [
  {
   "metadata": {},
   "source": [
    "\n<h2 id=\"CMSC-636,-HW4:-Deep-Restricted-Boltzman-Machine-on-MNIST\">CMSC 636, HW4: Deep Restricted Boltzman Machine on MNIST<a class=\"anchor-link\" href=\"#CMSC-636,-HW4:-Deep-Restricted-Boltzman-Machine-on-MNIST\">¶</a></h2>\n"
   ],
   "cell_type": "markdown"
  },
  {
   "metadata": {},
   "outputs": [],
   "source": [
    "\nimport numpy as np\nimport tensorflow as tf\nfrom types import SimpleNamespace\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom IPython.display import clear_output\n%matplotlib inline\n\n"
   ],
   "execution_count": 1,
   "cell_type": "code"
  },
  {
   "metadata": {},
   "source": [
    "\n<h2 id=\"1.-Load-MNIST\">1. Load MNIST<a class=\"anchor-link\" href=\"#1.-Load-MNIST\">¶</a></h2>\n"
   ],
   "cell_type": "markdown"
  },
  {
   "metadata": {},
   "outputs": [],
   "source": [
    "\n# load mnist dataset with labels encoded as one-hot vectors\nclass Dataset():\n    def __init__(self, data):\n        self.data = data\n        self.index = 0\n        self.epochs = 0\n\n    def shuffle(self):\n        perm = np.arange(self.data[0].shape[0])\n        np.random.shuffle(perm)\n        self.data = tuple(datai[perm] for datai in self.data)\n    \n    def next_batch(self, batch_size):\n        start = self.index\n        end = self.index + batch_size\n        if end > self.data[0].shape[0]:\n            self.epochs += 1\n            self.shuffle()\n            self.index, start = 0, 0\n            end = batch_size\n        self.index = end\n        return tuple(datai[start:end, ...] for datai in self.data)\n            \ndef load_mnist():\n    def preprocess(data, labels, num_classes):\n        # flatten images\n        data = data.astype(np.float32)/255.0\n        data = np.reshape(data, [data.shape[0], -1])\n        # one hot encoding\n        num_labels = labels.shape[0]\n        index_offset = np.arange(num_labels) * num_classes\n        labels_one_hot = np.zeros((num_labels, num_classes))\n        labels_one_hot.flat[index_offset + labels.ravel()] = 1\n        return data, labels_one_hot\n    train, test = tf.keras.datasets.mnist.load_data()\n    train = preprocess(train[0], train[1], 10)\n    test = preprocess(test[0], test[1], 10)\n    return SimpleNamespace(\n        train=Dataset(train), \n        test=Dataset(test))\nmnist = load_mnist()\n\n"
   ],
   "execution_count": null,
   "cell_type": "code"
  },
  {
   "metadata": {},
   "source": [
    "\n<h2 id=\"2.-Model-definition\">2. Model definition<a class=\"anchor-link\" href=\"#2.-Model-definition\">¶</a></h2>\n"
   ],
   "cell_type": "markdown"
  },
  {
   "metadata": {},
   "outputs": [],
   "source": [
    "\ndef bernoulli_sample_tf(x):\n    ''' sample from bernoulli distribution given a tf matrix '''\n    in_shape= x.get_shape().as_list()\n    uniform_samp = tf.random.uniform(shape=(in_shape[0], in_shape[1]), minval=0.0, maxval=1.0) \n    return tf.dtypes.cast(tf.greater(x, uniform_samp), tf.float32)\n\n"
   ],
   "execution_count": null,
   "cell_type": "code"
  },
  {
   "metadata": {},
   "source": [
    "\n<p>Implement the contrastive divergence update rule for the rbm, using a mini-batch of $M$ samples:</p>\n<p>$W \\leftarrow W + \\alpha \\; \\Delta W $ <br/>\n$b \\leftarrow b + \\alpha \\; \\Delta b $ <br/>\n$c \\leftarrow c + \\alpha \\; \\Delta c $ <br/></p>\n<p>Where:</p>\n<ul>\n<li>$ W \\in \\cal{R}^{V \\times H}, b \\in \\cal{R}^{H}, c \\in \\cal{R}^{V} $ <br/><br/></li>\n<li>$ X \\in \\cal{R}^{M \\times V}$ , is the matrix composed by the training samples in the mini-batch. Each row of $X$ is a training sample <br/><br/></li>\n<li>$ \\Delta W = \\dfrac{1}{M} \\left( X^T h(x) - X_s^T h(X_s) \\right)$ <br/><br/></li>\n<li>$ \\Delta b = {mean}(\\left( h(X) - h(X_s) \\right))$ <br/><br/></li>\n<li>$ \\Delta c = {mean}(\\left( X - X_s \\right))$ <br/><br/></li>\n<li>$ h(X) = \\sigma(X W + b) ; \\;\\;\\; \\sigma(x) = \\dfrac{1}{1+\\exp(-x)}$</li>\n</ul>\n"
   ],
   "cell_type": "markdown"
  },
  {
   "metadata": {},
   "outputs": [],
   "source": [
    "\nn_inputs=          # Complete: number of input units (i.e. V)\nn_hidden= [ 20, 20]    # number of hidden units (i.e. H), try different values\nbatch_size= 500         # number of samples on the mini-batch (i.e. M)\n\nk= 5          #Try different values \nalpha= 0.1  #Try different values\n\n"
   ],
   "execution_count": null,
   "cell_type": "code"
  },
  {
   "metadata": {},
   "outputs": [],
   "source": [
    "\nclass rbm_model(object):\n    def __init__(self, n_inputs, n_hidden, w_stddev=0.1, k = 1, alpha = 0.1, batch_size = 500):\n        ''''Define the parameters of the rbm model'''\n        self.w = # Complete: weight matrix for the rbm model\n        self.b = # Complete: b vector for the rbm model\n        \n        self.c = # Complete: c vector for the rbm model\n        \n        self.k = k\n        \n        self.alpha = alpha\n        \n        self.batch_size = batch_size\n        \n        self.n_hidden = n_hidden\n    \n    def __call__(self, x):\n        '''Model function definition'''\n        \n        # K gibbs sampling\n        xs = x\n        for i in range(self.k):\n            h_prob = tf.sigmoid(tf.matmul(xs, self.w) + self.b)\n            hs = bernoulli_sample_tf(h_prob)\n            x_prob = tf.sigmoid(tf.matmul(hs, tf.transpose(self.w)) + self.c)\n            xs = bernoulli_sample_tf(x_prob)\n            \n        # Parameter update:\n        h_x =   # Complete: compute h(X)\n        h_xs =  # Complete: compute h(Xs)\n        \n        dw =    # Complete: follow update equations\n     \n        \n        db =    # Complete: follow update equations\n        dc =    # Complete: follow update equations\n\n        \n        op_W= self.w.assign_add( self.alpha*dw )\n        op_b= self.b.assign_add( self.alpha*db )\n        op_c= self.c.assign_add( self.alpha*dc )\n        \n        return x_prob, h_x\n        \nlayer0 = rbm_model(n_inputs = n_inputs, n_hidden = n_hidden[0], w_stddev=0.1, k = k, alpha = alpha, batch_size = batch_size)\nlayer1 = rbm_model(n_inputs = n_hidden[0], n_hidden = n_hidden[1], w_stddev=0.1, k = k, alpha = alpha, batch_size = batch_size)\n\n"
   ],
   "execution_count": null,
   "cell_type": "code"
  },
  {
   "metadata": {},
   "source": [
    "\n<h1 id=\"Training\">Training<a class=\"anchor-link\" href=\"#Training\">¶</a></h1>\n"
   ],
   "cell_type": "markdown"
  },
  {
   "metadata": {},
   "source": [
    "\n<h3 id=\"Training-of-layer-1\">Training of layer 1<a class=\"anchor-link\" href=\"#Training-of-layer-1\">¶</a></h3>\n"
   ],
   "cell_type": "markdown"
  },
  {
   "metadata": {},
   "outputs": [],
   "source": [
    "\nnum_steps= 10000 # try different values\nn_logging = 500  # try different values\n\nfor step in range(num_steps):\n    # -------- train RBM -----#\n    batch_x, _= mnist.train.next_batch(batch_size)\n    x_g, h1_x = layer0(batch_x)\n    \n    \n    \n    \n    #  ------- logging -------\n    if step%n_logging == 0:\n        clear_output()\n        print('Samples generated starting from given X')\n        \n        x_gaux = np.reshape(x_g, [-1,28,28,1])        \n        plt.figure(1)\n        for i in range(2*4):\n            plt.subplot(241 + i)\n            plt.imshow(x_gaux[i,:,:,0], cmap='Greys_r')\n        plt.show()\n        \n        print('Samples generated from random H1')\n        \n        \n        # random samples from RBM model starting with random h1 samples\n        hst = tf.random.uniform(shape=(layer0.batch_size, layer0.n_hidden), minval=0, maxval=1)\n        \n        for i in range(layer0.k): \n            Xst_prob = tf.sigmoid(tf.matmul(hst, tf.transpose(layer0.w)) + layer0.c)\n            Xst = bernoulli_sample_tf(Xst_prob)\n            hst_prob = tf.sigmoid(tf.matmul(Xst, layer0.w) + layer0.b)\n            hst = bernoulli_sample_tf(hst_prob)\n        x_g = tf.sigmoid(tf.matmul(hst, tf.transpose(layer0.w)) + layer0.c) # X generated from random h1\n        \n        \n        x_gaux = np.reshape(x_g, [-1,28,28,1])\n        \n        plt.figure(2)\n        for i in range(2*4):\n            plt.subplot(241 + i)\n            plt.imshow(x_gaux[i,:,:,0], cmap='Greys_r')\n        plt.show()\n\n"
   ],
   "execution_count": null,
   "cell_type": "code"
  },
  {
   "metadata": {},
   "source": [
    "\n<h3 id=\"Training-of-layer-2\">Training of layer 2<a class=\"anchor-link\" href=\"#Training-of-layer-2\">¶</a></h3>\n"
   ],
   "cell_type": "markdown"
  },
  {
   "metadata": {},
   "outputs": [],
   "source": [
    "\nnum_steps= 10000 # try different values\nn_logging = 500  # try different values\n\nfor step in range(num_steps):\n    # -------- train RBM -----#\n    batch_x, _= mnist.train.next_batch(batch_size)\n    x_g, h1_x = layer0(batch_x)\n    h1s_prob, h2_h1 = layer1(h1_x)\n    x_g = tf.sigmoid(tf.matmul(h1s_prob, tf.transpose(layer0.w)) + layer0.c) # samples given X while training second RBM\n\n    #  ------- logging -------\n    if step%n_logging == 0:\n        clear_output()\n        print('Samples generated starting from given X')\n        x_gaux = np.reshape(x_g, [-1,28,28,1])\n        \n        plt.figure(1)\n        for i in range(2*4):\n            plt.subplot(241 + i)\n            plt.imshow(x_gaux[i,:,:,0], cmap='Greys_r')\n        plt.show()\n        \n        print('Samples generated from random H2')\n        \n        \n        # random samples from RBM model starting with random h2 samples\n        h2st = tf.random.uniform(shape=(layer1.batch_size, layer1.n_hidden), minval=0, maxval=1)\n        for i in range(layer1.k): \n            h1st_prob = tf.sigmoid(tf.matmul(h2st, tf.transpose(layer1.w)) + layer1.c)\n            h1st = bernoulli_sample_tf(h1st_prob)\n            h2st_prob = tf.sigmoid(tf.matmul(h1st, layer1.w) + layer1.b)\n            h2st = bernoulli_sample_tf(h2st_prob)\n        x_g = tf.sigmoid(tf.matmul(h1st, tf.transpose(layer0.w)) + layer0.c) # X generated from random h2\n        \n        x_gaux = np.reshape(x_g, [-1,28,28,1])\n        \n        plt.figure(2)\n        for i in range(2*4):\n            plt.subplot(241 + i)\n            plt.imshow(x_gaux[i,:,:,0], cmap='Greys_r')\n        plt.show()\n        \n\n"
   ],
   "execution_count": null,
   "cell_type": "code"
  },
  {
   "metadata": {},
   "outputs": [],
   "source": [
    "\n \n\n"
   ],
   "execution_count": null,
   "cell_type": "code"
  }
 ],
 "metadata": {}
}