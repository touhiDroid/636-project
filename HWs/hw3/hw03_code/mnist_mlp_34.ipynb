{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"CMSC-636,-HW3:-Multilayer-perceptron-on-MNIST-dataset\">CMSC 636, HW3: Multilayer perceptron on MNIST dataset<a class=\"anchor-link\" href=\"#CMSC-636,-HW3:-Multilayer-perceptron-on-MNIST-dataset\">¶</a></h1><h3 id=\"NOTE:-Do-not-use-the-Keras-library\">NOTE: Do not use the Keras library<a class=\"anchor-link\" href=\"#NOTE:-Do-not-use-the-Keras-library\">¶</a></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from types import SimpleNamespace\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"1.-Load-MNIST\">1. Load MNIST<a class=\"anchor-link\" href=\"#1.-Load-MNIST\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load mnist dataset with labels encoded as one-hot vectors\n",
    "class Dataset():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.index = 0\n",
    "        self.epochs = 0\n",
    "\n",
    "    def shuffle(self):\n",
    "        perm = np.arange(self.data[0].shape[0])\n",
    "        np.random.shuffle(perm)\n",
    "        self.data = tuple(datai[perm] for datai in self.data)\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        start = self.index\n",
    "        end = self.index + batch_size\n",
    "        if end > self.data[0].shape[0]:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "            self.index, start = 0, 0\n",
    "            end = batch_size\n",
    "        self.index = end\n",
    "        return tuple(datai[start:end, ...] for datai in self.data)\n",
    "            \n",
    "def load_mnist():\n",
    "    def preprocess(data, labels, num_classes):\n",
    "        # flatten images\n",
    "        data = data.astype(np.float32)/255.0\n",
    "        data = np.reshape(data, [data.shape[0], -1])\n",
    "        # one hot encoding\n",
    "        num_labels = labels.shape[0]\n",
    "        index_offset = np.arange(num_labels) * num_classes\n",
    "        labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "        labels_one_hot.flat[index_offset + labels.ravel()] = 1\n",
    "        return data, labels_one_hot\n",
    "    train, test = tf.keras.datasets.mnist.load_data()\n",
    "    train = preprocess(train[0], train[1], 10)\n",
    "    test = preprocess(test[0], test[1], 10)\n",
    "    return SimpleNamespace(\n",
    "        train=Dataset(train), \n",
    "        test=Dataset(test))\n",
    "mnist = load_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"2.-Model-definition\">2. Model definition<a class=\"anchor-link\" href=\"#2.-Model-definition\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "# Define parameters of the network\n",
    "input_size= 28*28\n",
    "n_outputs =  10\n",
    "n_hidden = 100\n",
    "batch_size= 100\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Define the model:</p>\n",
    "<p>h1 = relu(x w1 + b1) &lt;/br&gt;\n",
    "logits = h1 w2 + b2 &lt;/br&gt;</p>\n",
    "<p>Where \"x w1\" is a matrix multiplication between the matices x and w1. \n",
    "The matrix x is a matrix whose rows represent the training input data.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DenseLayer(object):\n",
    "    def __init__(self, n_inputs, n_units, afunc=None, w_stddev=0.01):\n",
    "        '''Define the parameters of the layer'''\n",
    "        self.w = tf.Variable(\n",
    "            tf.random.truncated_normal([n_inputs, n_units], stddev=w_stddev),\n",
    "            name='w')\n",
    "        self.b = tf.Variable(\n",
    "            tf.zeros([n_units]), \n",
    "            name='b')\n",
    "        self.afunc = afunc\n",
    "    \n",
    "    def trainable_variables(self):\n",
    "        '''return trainable variables'''\n",
    "        return [self.w, self.b]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        '''Layer function definition'''\n",
    "        y = tf.matmul(x, self.w) + self.b\n",
    "        if self.afunc is not None:\n",
    "            y = self.afunc(y)\n",
    "        return y\n",
    "\n",
    "class LogisticReg(object):\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def trainable_variables(self):\n",
    "        return [var for layer in self.layers \n",
    "                for var in layer.trainable_variables()]\n",
    "        \n",
    "    def __call__(self, x, logits=False):\n",
    "        '''call layers and apply softmax if logits=False'''\n",
    "        # compute layers\n",
    "        output = x\n",
    "        for layer in self.layers:\n",
    "            output = layer(output)\n",
    "        # apply softmax if logits is false\n",
    "        # use logits=True for training\n",
    "        if not logits:  \n",
    "            output = tf.nn.softmax(output)\n",
    "        return output\n",
    "\n",
    "def loss_fn(logits, labels, weights):\n",
    "    '''compute softmax cross entory'''\n",
    "    error = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, \n",
    "        labels=labels))\n",
    "    # weights are not used... try to add a regularization loss\n",
    "    # reg = ???\n",
    "    reg = tf.reduce_mean([tf.nn.l2_loss(w) for w in weights])\n",
    "    return error + 0.0001*reg\n",
    "    \n",
    "# define model\n",
    "hidden_layer = DenseLayer(n_inputs=784, n_units=n_hidden, afunc=tf.nn.sigmoid)\n",
    "hidden_layer2 = DenseLayer(n_inputs=n_hidden, n_units=n_hidden, afunc=tf.nn.sigmoid)\n",
    "out_layer = DenseLayer(n_inputs=n_hidden, n_units=n_outputs)\n",
    "model = LogisticReg([hidden_layer, hidden_layer2, out_layer])\n",
    "weights = [layer.w for layer in model.layers]\n",
    "\n",
    "# define optimizer.\n",
    "optimizer = tf.keras.optimizers.Adam(1e-3)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"3.-Training\">3. Training<a class=\"anchor-link\" href=\"#3.-Training\">¶</a></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    if n_outputs == 1:\n",
    "        return (100.0 * np.sum(np.greater(predictions, 0.5) == np.greater(labels, 0.5))/ predictions.shape[0])\n",
    "    else:\n",
    "        return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))/ predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 10000\n",
    "summary_freq = 200\n",
    "n_test_log = 10\n",
    "\n",
    "def train_step(labels, inputs):\n",
    "    '''run a single step of gradient descent'''\n",
    "    # compute gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(inputs, logits=True)\n",
    "        loss = loss_fn(logits, labels, weights)\n",
    "    # apply grandients to optimizer\n",
    "    gradients = tape.gradient(loss, model.trainable_variables())\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables()))\n",
    "    \n",
    "    return loss.numpy(), model(inputs).numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 , train: 0.065  | test: 8.3  | loss: 0.011538406610488891\n",
      "200 , train: 42.555  | test: 73.6  | loss: 1.7318273410201073\n",
      "400 , train: 81.135  | test: 86.5  | loss: 0.7238972349464894\n",
      "600 , train: 88.25  | test: 89.4  | loss: 0.4707300987839699\n",
      "800 , train: 90.84  | test: 91.4  | loss: 0.36203978665173053\n",
      "1000 , train: 92.11  | test: 93.3  | loss: 0.3072360014915466\n",
      "1200 , train: 93.105  | test: 92.1  | loss: 0.272859590575099\n",
      "1400 , train: 93.99  | test: 95.4  | loss: 0.24536207787692546\n",
      "1600 , train: 94.565  | test: 95.7  | loss: 0.22538544051349163\n",
      "1800 , train: 94.97  | test: 95.3  | loss: 0.20754133701324462\n",
      "2000 , train: 95.18  | test: 94.3  | loss: 0.2001122620329261\n",
      "2200 , train: 95.86  | test: 94.1  | loss: 0.1804575649648905\n",
      "2400 , train: 95.825  | test: 96.6  | loss: 0.17401244297623633\n",
      "2600 , train: 96.16  | test: 95.7  | loss: 0.17036365266889333\n",
      "2800 , train: 96.585  | test: 95.5  | loss: 0.15566566694527864\n",
      "3000 , train: 96.745  | test: 95.6  | loss: 0.1461813496053219\n",
      "3200 , train: 96.715  | test: 95.5  | loss: 0.1515293715149164\n",
      "3400 , train: 97.115  | test: 97.1  | loss: 0.13785521240904927\n",
      "3600 , train: 97.165  | test: 97.5  | loss: 0.13570913255214692\n",
      "3800 , train: 97.355  | test: 97.0  | loss: 0.1324633192643523\n",
      "4000 , train: 97.545  | test: 97.0  | loss: 0.12716396212577818\n",
      "4200 , train: 97.565  | test: 96.3  | loss: 0.1207137930765748\n",
      "4400 , train: 97.62  | test: 97.0  | loss: 0.12040609493851662\n",
      "4600 , train: 97.79  | test: 97.4  | loss: 0.11510484931990504\n",
      "4800 , train: 98.03  | test: 97.0  | loss: 0.10863587884232402\n",
      "5000 , train: 97.825  | test: 96.7  | loss: 0.11235811233520508\n",
      "5200 , train: 98.195  | test: 97.0  | loss: 0.1039209657907486\n",
      "5400 , train: 98.095  | test: 97.3  | loss: 0.10498233020305633\n",
      "5600 , train: 98.22  | test: 96.6  | loss: 0.10310672169551253\n",
      "5800 , train: 98.365  | test: 96.6  | loss: 0.09916820967569948\n",
      "6000 , train: 98.41  | test: 97.0  | loss: 0.09773184515535832\n",
      "6200 , train: 98.41  | test: 98.0  | loss: 0.0946595243178308\n",
      "6400 , train: 98.775  | test: 97.4  | loss: 0.08609703909605741\n",
      "6600 , train: 98.43  | test: 97.2  | loss: 0.0950231715850532\n",
      "6800 , train: 98.525  | test: 97.3  | loss: 0.09313816219568252\n",
      "7000 , train: 98.89  | test: 97.6  | loss: 0.0847415741533041\n",
      "7200 , train: 98.815  | test: 97.8  | loss: 0.0840737035125494\n",
      "7400 , train: 98.69  | test: 97.5  | loss: 0.08871536729857325\n",
      "7600 , train: 98.925  | test: 97.6  | loss: 0.08183810157701372\n",
      "7800 , train: 98.89  | test: 96.9  | loss: 0.08273108001798392\n",
      "8000 , train: 98.865  | test: 97.7  | loss: 0.08035411026328802\n",
      "8200 , train: 99.06  | test: 98.6  | loss: 0.0770528326742351\n",
      "8400 , train: 99.055  | test: 97.2  | loss: 0.08104117585346103\n",
      "8600 , train: 99.1  | test: 97.6  | loss: 0.07584768081083894\n",
      "8800 , train: 99.315  | test: 97.1  | loss: 0.07169991755858064\n",
      "9000 , train: 99.05  | test: 97.1  | loss: 0.07637742694467306\n",
      "9200 , train: 99.155  | test: 98.1  | loss: 0.07452781012281776\n",
      "9400 , train: 99.385  | test: 97.7  | loss: 0.06965800985693932\n",
      "9600 , train: 99.235  | test: 98.2  | loss: 0.07210672548040747\n",
      "9800 , train: 99.19  | test: 97.3  | loss: 0.07373605350032449\n",
      "10000 , train: 99.475  | test: 97.2  | loss: 0.06665678068995476\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mean_loss= 0\n",
    "train_accuracy= 0\n",
    "# Perform num_steps training steps\n",
    "for step in range(num_steps + 1):\n",
    "    # Get next batch of 100 images\n",
    "    batch_X, batch_y= mnist.train.next_batch(batch_size)    \n",
    "    # Call the optimizer to perform one step of the training\n",
    "    l, train_pred = train_step(batch_y, batch_X)\n",
    "    # Compute accuracy\n",
    "    train_accuracy += accuracy(train_pred, batch_y)\n",
    "    mean_loss += l\n",
    "    if step%summary_freq == 0:\n",
    "        # Mean train accuracy\n",
    "        train_accuracy= train_accuracy/summary_freq\n",
    "        # Evaluate accuracy on test dataset\n",
    "        test_accuracy= 0\n",
    "        for i in range(n_test_log):\n",
    "            batch_X_test, batch_y_test= mnist.test.next_batch(batch_size) \n",
    "            pred = model(batch_X_test)\n",
    "            test_accuracy += accuracy(pred, batch_y_test)\n",
    "        test_accuracy= test_accuracy/n_test_log\n",
    "        \n",
    "        print(step, ', train:',train_accuracy,' | test:', test_accuracy, ' | loss:', mean_loss/summary_freq)\n",
    "        mean_loss= 0\n",
    "        train_accuracy= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"4.-Test-the-model-using-testing-dataset\">4. Test the model using testing dataset<a class=\"anchor-link\" href=\"#4.-Test-the-model-using-testing-dataset\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual number: 8\n",
      "Prediction by the model: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7f9f501bf2e0>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Acquire one sample from the mnist dataset\n",
    "test_sample_x, test_sample_y= mnist.test.next_batch(1) \n",
    "\n",
    "# Get a prediction for this sample\n",
    "pred = model(test_sample_x, logits=False)\n",
    "\n",
    "print('Actual number:', np.argmax(test_sample_y))\n",
    "print('Prediction by the model:', np.argmax(pred))\n",
    "\n",
    "# plot\n",
    "plt.imshow(np.reshape(test_sample_x, [28,28]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}