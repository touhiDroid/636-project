{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "<h1 id=\"CMSC-636,-HW3:-ConvNet-on-MNIST-dataset\">CMSC 636, HW3: ConvNet on MNIST dataset<a class=\"anchor-link\" href=\"#CMSC-636,-HW3:-ConvNet-on-MNIST-dataset\">¶</a></h1><h3 id=\"NOTE:-Do-not-use-the-Keras-library\">NOTE: Do not use the Keras library<a class=\"anchor-link\" href=\"#NOTE:-Do-not-use-the-Keras-library\">¶</a></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from types import SimpleNamespace\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Classes from previous exercises\n",
    "class DenseLayer(object):\n",
    "    def __init__(self, n_inputs, n_units, afunc=None, w_stddev=0.01):\n",
    "        \"\"\"Define the parameters of the layer\"\"\"\n",
    "        self.w = tf.Variable(\n",
    "            tf.random.truncated_normal([n_inputs, n_units], stddev=w_stddev),\n",
    "            name='w')\n",
    "        self.b = tf.Variable(\n",
    "            tf.zeros([n_units]), \n",
    "            name='b')\n",
    "        self.afunc = afunc\n",
    "    \n",
    "    def trainable_variables(self):\n",
    "        \"\"\"return trainable variables\"\"\"\n",
    "        return [self.w, self.b]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"Layer function definition\"\"\"\n",
    "        y = tf.matmul(x, self.w) + self.b\n",
    "        if self.afunc is not None:\n",
    "            y = self.afunc(y)\n",
    "        return y\n",
    "\n",
    "class LogisticReg(object):\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def trainable_variables(self):\n",
    "        return [var for layer in self.layers \n",
    "                for var in layer.trainable_variables()]\n",
    "        \n",
    "    def __call__(self, x, logits=False):\n",
    "        \"\"\"call layers and apply softmax if logits=False\"\"\"\n",
    "        # compute layers\n",
    "        output = x\n",
    "        for layer in self.layers:\n",
    "            output = layer(output)\n",
    "        # apply softmax if logits is false\n",
    "        # use logits=True for training\n",
    "        if not logits:  \n",
    "            output = tf.nn.softmax(output)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"1.-Load-MNIST\">1. Load MNIST<a class=\"anchor-link\" href=\"#1.-Load-MNIST\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load mnist dataset with labels encoded as one-hot vectors\n",
    "class Dataset(object):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.index = 0\n",
    "        self.epochs = 0\n",
    "\n",
    "    def shuffle(self):\n",
    "        perm = np.arange(self.data[0].shape[0])\n",
    "        np.random.shuffle(perm)\n",
    "        self.data = tuple(datai[perm] for datai in self.data)\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        start = self.index\n",
    "        end = self.index + batch_size\n",
    "        if end > self.data[0].shape[0]:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "            self.index, start = 0, 0\n",
    "            end = batch_size\n",
    "        self.index = end\n",
    "        return tuple(datai[start:end, ...] for datai in self.data)\n",
    "            \n",
    "def load_mnist():\n",
    "    def preprocess(data, labels, num_classes):\n",
    "        # flatten images\n",
    "        data = data.astype(np.float32)/255.0\n",
    "        data = np.reshape(data, [data.shape[0], -1])\n",
    "        # one hot encoding\n",
    "        num_labels = labels.shape[0]\n",
    "        index_offset = np.arange(num_labels) * num_classes\n",
    "        labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "        labels_one_hot.flat[index_offset + labels.ravel()] = 1\n",
    "        return data, labels_one_hot\n",
    "    train, test = tf.keras.datasets.mnist.load_data()\n",
    "    train = preprocess(train[0], train[1], 10)\n",
    "    test = preprocess(test[0], test[1], 10)\n",
    "    return SimpleNamespace(\n",
    "        train=Dataset(train), \n",
    "        test=Dataset(test))\n",
    "mnist = load_mnist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"2.-Model-definition\">2. Model definition<a class=\"anchor-link\" href=\"#2.-Model-definition\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-15 16:30:34.899837: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ConvLayer(object):\n",
    "    def __init__(self, input_maps, output_maps, filter_size, \n",
    "                 pool_size, afunc=None):\n",
    "        \"\"\"\n",
    "        Convolution layer with VALID padding and pooling layer.\n",
    "\n",
    "        input_maps: number of input maps.\n",
    "        output_maps: number of output maps.\n",
    "        filter_size: list/tuple with the size of the kernel filter.\n",
    "        pool_size: list/tuple with the size of the pool filter.\n",
    "        afunc: activation function.\n",
    "        \"\"\"\n",
    "        self.w = tf.Variable(\n",
    "            tf.random.truncated_normal(\n",
    "                shape=[filter_size[0], filter_size[1], input_maps, output_maps], stddev=0.1),\n",
    "            name='w')\n",
    "        self.b = tf.Variable(\n",
    "            tf.random.truncated_normal(shape=[output_maps], stddev=0.1),\n",
    "            name= 'b')\n",
    "        self.pool_size = pool_size\n",
    "        self.afunc = afunc\n",
    "    \n",
    "    def trainable_variables(self):\n",
    "        \"\"\"return trainable variables\"\"\"\n",
    "        return [self.w, self.b]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        out = tf.nn.conv2d(x, self.w, strides=[1, 1, 1, 1], padding='VALID')\n",
    "        out = out + self.b\n",
    "        if self.afunc is not None:\n",
    "            out = self.afunc(out)\n",
    "        out = tf.nn.max_pool(out, ksize=[1, self.pool_size[0], self.pool_size[1], 1],\n",
    "            strides=[1, self.pool_size[0], self.pool_size[1], 1], padding='VALID')\n",
    "        return out\n",
    "\n",
    "class ReshapeLayer(object):\n",
    "    def __init__(self, output_shape):\n",
    "        self.output_shape = output_shape\n",
    "    \n",
    "    def trainable_variables(self):\n",
    "        \"\"\"return trainable variables\"\"\"\n",
    "        return [] # '''------- COMPLETE -------'''\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return tf.reshape(x, self.output_shape)\n",
    "\n",
    "\n",
    "def loss_fn(logits, labels, weights):\n",
    "    \"\"\"compute softmax cross entory\"\"\"\n",
    "    error = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, \n",
    "        labels=labels))\n",
    "    # add an l2 regularization loss\n",
    "    reg = tf.reduce_mean([tf.nn.l2_loss(w) for w in weights]) # '''------- COMPLETE -------'''\n",
    "    return error + 0.0001*reg\n",
    "    \n",
    "# define model\n",
    "model = LogisticReg([\n",
    "    ConvLayer(1, 32, (5,5), (2,2), tf.nn.relu), # look at the code above to\n",
    "    # understand the parameters\n",
    "    ConvLayer(32, 64, (5,5), (2,2), tf.nn.relu),\n",
    "    ReshapeLayer([-1, 4*4*64]), # HINT: where does 4*4*64 comes from?\n",
    "    # use the architecture given in the\n",
    "    # assignment to figure this out !!!\n",
    "    # Answer: The width of the output of\n",
    "    # first conv2d is calculated using (W−F+2P)/S+1 # (28-5+2*0)/1+1= 24 and then the output of\n",
    "    # maxpool is 24/2 = 12, then the output of second # conv2d is (12-5+2*0)/1+1=8, then the output of # maxpool is 8/2 = 4 and the same happens for the # height so the outcome of second ConvLayer\n",
    "    # is W*H*output_channel = 4*4*64\n",
    "    DenseLayer(4*4*64, 256, tf.nn.relu),\n",
    "    DenseLayer(256, 10)\n",
    "]) # COMPLETE\n",
    "weights = [layer.w for layer in model.layers if hasattr(layer, 'w')]\n",
    "\n",
    "# define optimizer.\n",
    "optimizer = tf.keras.optimizers.Adam(1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"3.-Training\">3. Training<a class=\"anchor-link\" href=\"#3.-Training\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 5000     # play with different values\n",
    "summary_freq = 200\n",
    "n_test_log = 10\n",
    "n_outputs = 10\n",
    "batch_size= 100 # play with different values for batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    if n_outputs == 1:\n",
    "        return (100.0 * np.sum(np.greater(predictions, 0.5) == np.greater(labels, 0.5))/ predictions.shape[0])\n",
    "    else:\n",
    "        return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))/ predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_step(labels, inputs):\n",
    "    \"\"\"run a single step of gradient descent\"\"\"\n",
    "    # compute gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(inputs, logits=True)\n",
    "        loss = loss_fn(logits, labels, weights)\n",
    "    # apply grandients to optimizer\n",
    "    gradients = tape.gradient(loss, model.trainable_variables())\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables()))\n",
    "\n",
    "    return loss.numpy(), model(inputs).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mean_loss= 0\n",
    "train_accuracy= 0\n",
    "for step in range(num_steps):\n",
    "    # Get next batch of 100 images\n",
    "    batch_X, batch_y= mnist.train.next_batch(batch_size)\n",
    "    # The images returned by the function are formated in a matrix,\n",
    "    # where each row represents an image. Hence, we must reshape such\n",
    "    # matrix to convert the vector-representation of the images to\n",
    "    # standard 28 by 28 grey images.\n",
    "    batch_X= np.reshape(batch_X, [-1,28,28,1])\n",
    "\n",
    "    # Call the optimizer to perform one step of the training\n",
    "    l, train_pred = train_step(batch_y, batch_X)\n",
    "    # Compute accuracy\n",
    "    train_accuracy += accuracy(train_pred, batch_y)\n",
    "    mean_loss += l\n",
    "\n",
    "    if step%summary_freq == 0:\n",
    "        # obtain train accuracy\n",
    "        train_accuracy= train_accuracy/summary_freq\n",
    "\n",
    "        ''' ------- YOUR CODE HERE -------- '''\n",
    "        # Evaluate the test accuracy on a series of mini-batches\n",
    "        # extracted from the testing dataset.\n",
    "        # Use mini-batches of around ~100 images\n",
    "        test_accuracy= 0\n",
    "        for i in range(n_test_log):\n",
    "            batch_X_test, batch_y_test= mnist.test.next_batch(batch_size)\n",
    "            batch_X_test= np.reshape(batch_X_test, [-1,28,28,1])\n",
    "            pred = model(batch_X_test)\n",
    "            test_accuracy += accuracy(pred, batch_y_test)\n",
    "        test_accuracy= test_accuracy/n_test_log\n",
    "\n",
    "        # ------------------------------- #\n",
    "        print(step, ', train:',train_accuracy,' | test:', test_accuracy, ' | loss:', mean_loss/summary_freq)\n",
    "        mean_loss= 0\n",
    "        train_accuracy= 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"4.-Test-the-trained-model-on-the-testing-dataset\">4. Test the trained model on the testing dataset<a class=\"anchor-link\" href=\"#4.-Test-the-trained-model-on-the-testing-dataset\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Acquire one sample from the mnist dataset\n",
    "test_sample_x, test_sample_y= mnist.test.next_batch(1) \n",
    "\n",
    "''' ------- YOUR CODE HERE -------- '''\n",
    "# Evaluate the training model on test_sample_x\n",
    "# and compare it with the actual label test_sample_y\n",
    "test_sample_x= np.reshape(test_sample_x, [-1,28,28,1])\n",
    "pred = model(test_sample_x)\n",
    "print('Number:', np.argmax(test_sample_y))\n",
    "print('Prediction by the model:', np.argmax(pred))\n",
    "# ------------------------------- #\n",
    "\n",
    "# Plot\n",
    "plt.imshow(np.squeeze(test_sample_x))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}